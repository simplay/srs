"use strict";(self.webpackChunksrs=self.webpackChunksrs||[]).push([[3237],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=o,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return n?a.createElement(h,r(r({ref:t},c),{},{components:n})):a.createElement(h,r({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},3056:(e,t,n)=>{n.d(t,{ZP:()=>r});var a=n(7462),o=(n(7294),n(3905));const i={toc:[{value:"Building and running the pipeling locally in DEV",id:"building-and-running-the-pipeling-locally-in-dev",level:2},{value:"Building and running the pipeline on the Test Machine",id:"building-and-running-the-pipeline-on-the-test-machine",level:2},{value:"Building and running the pipeline on the Prod Machine",id:"building-and-running-the-pipeline-on-the-prod-machine",level:2},{value:"Initial server IP configuration",id:"initial-server-ip-configuration",level:3},{value:"Deployment",id:"deployment",level:3},{value:"Accessing the applications once the services are up",id:"accessing-the-applications-once-the-services-are-up",level:2},{value:"Load testing",id:"load-testing",level:2},{value:"Configure an experiment and record data",id:"configure-an-experiment-and-record-data",level:2},{value:"Exporting data",id:"exporting-data",level:2},{value:"Exporting data using the SensorRecordingSoftwareFrontend",id:"exporting-data-using-the-sensorrecordingsoftwarefrontend",level:3},{value:"Exporting data using the export script",id:"exporting-data-using-the-export-script",level:3},{value:"Importing some dumps",id:"importing-some-dumps",level:2},{value:"Chronograf authentication",id:"chronograf-authentication",level:2},{value:"TICK Stack configuration",id:"tick-stack-configuration",level:2},{value:"Kapacitor alerts and notifications",id:"kapacitor-alerts-and-notifications",level:2},{value:"Seeing outputs (logs) of the Sensors srs backend or Nginx",id:"seeing-outputs-logs-of-the-sensors-srs-backend-or-nginx",level:2},{value:"Process",id:"process",level:2},{value:"Note on deployment strategy",id:"note-on-deployment-strategy",level:3},{value:"CI/CD setup",id:"cicd-setup",level:2},{value:"Creating the user for deployment",id:"creating-the-user-for-deployment",level:3},{value:"Register a gitlab runner",id:"register-a-gitlab-runner",level:3},{value:"Configure the Gitlab CI/CD variables",id:"configure-the-gitlab-cicd-variables",level:3},{value:"Using Gitlab-CI pipelines for the deployment",id:"using-gitlab-ci-pipelines-for-the-deployment",level:2},{value:"Rollback to a previous version using Gitlab-CI",id:"rollback-to-a-previous-version-using-gitlab-ci",level:2},{value:"Cleanup on the production machine",id:"cleanup-on-the-production-machine",level:2},{value:"Strategy",id:"strategy",level:2},{value:"Daily backup script",id:"daily-backup-script",level:2},{value:"Backup job set up on Synology NAS (Rainbow_Dash)",id:"backup-job-set-up-on-synology-nas-rainbow_dash",level:2},{value:"Pre-requisite: add a dedicated user for the backup on the production machine",id:"pre-requisite-add-a-dedicated-user-for-the-backup-on-the-production-machine",level:3},{value:"Configure the backup job on the NAS",id:"configure-the-backup-job-on-the-nas",level:3},{value:"Restoration from a daily backup archive",id:"restoration-from-a-daily-backup-archive",level:2},{value:"Coding guidelines",id:"coding-guidelines",level:2},{value:"Docker",id:"docker",level:2},{value:"Working with MongoDB container",id:"working-with-mongodb-container",level:2}]};function r(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,a.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"sensor-recording-software-srs"},"Sensor Recording Software (SRS)"),(0,o.kt)("p",null,"This repository is used to build and deploy the ",(0,o.kt)("inlineCode",{parentName:"p"},"SRS")," pipeline to create a sensor system. Preview Version"),(0,o.kt)("p",null,"The Pipeline is composed of the following components:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Reverse Proxy"),": the reverse proxy acts as the entry point of the backend and route the requests to the various components."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://gitlab.com/srs/srs-backend"},(0,o.kt)("inlineCode",{parentName:"a"},"SensorRecordingSoftwareBackend"))," (modules/srs-backend): service that exposes a REST API to receive data from sensors, websockets to control some of them (like the camera) and save the data in a ",(0,o.kt)("inlineCode",{parentName:"li"},"MongoDB"),"."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://gitlab.com/srs/srs-frontend"},(0,o.kt)("inlineCode",{parentName:"a"},"SensorRecordingSoftwareFrontend"))," (modules/srs-frontend): web application (nuxt) to monitor sensors and perform some actions on the pipeline."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"MongoDB"),": a Mongo database that stores the measurements (raw data)."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"DataExtractor"),": a Python application triggered by the ",(0,o.kt)("inlineCode",{parentName:"li"},"SensorRecordingSoftwareBackend")," to export raw data from ",(0,o.kt)("inlineCode",{parentName:"li"},"MongoDB")," as ",(0,o.kt)("inlineCode",{parentName:"li"},"Apache Parquet")," files."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"TICK Stack"),": a monitoring tool based on ",(0,o.kt)("inlineCode",{parentName:"li"},"InfluxData")," TICK Stack.")),(0,o.kt)("p",null,"The InfluxData TICK stack is composed of the following modules:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"T"),"elegraf: agent for collecting and reporting metrics and events"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"I"),"nfluxDB: the time series database"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"C"),"hronograph: the interface to manage the entire InfluxData platform"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"K"),"apacitor: real-time streaming data processing engine, also used for alerting")),(0,o.kt)("p",null,"Detailed information about each module of the TICK stack can be found ",(0,o.kt)("a",{parentName:"p",href:"https://www.influxdata.com/time-series-platform/"},"here"),"."),(0,o.kt)("h1",{id:"software-prerequisites"},"Software Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"docker"),": ",(0,o.kt)("a",{parentName:"p",href:"https://docs.docker.com/engine/install/ubuntu/"},"https://docs.docker.com/engine/install/ubuntu/"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"docker-compose"),": ",(0,o.kt)("a",{parentName:"p",href:"https://docs.docker.com/compose/install/"},"https://docs.docker.com/compose/install/"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"k6")," (required for load testing). Installation: ",(0,o.kt)("a",{parentName:"p",href:"https://k6.io/docs/getting-started/installation/"},"https://k6.io/docs/getting-started/installation/"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"python3")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"bcrypt")," (required to inject users and their credentials in DB). Installation:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip install bcrypt\n")))),(0,o.kt)("h1",{id:"initial-setup"},"Initial Setup"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"After having cloned the repo, initialize its submodules:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"git submodule update --init --recursive\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the ",(0,o.kt)("inlineCode",{parentName:"p"},".env")," file for development:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"cp README.env.dev .env.dev\n")),(0,o.kt)("p",{parentName:"li"},"Adapt the values in the env file if needed.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the ",(0,o.kt)("inlineCode",{parentName:"p"},".env")," file for the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareFrontend"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"cp modules/srs-frontend/.env.README modules/srs-frontend/.env\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the JWT certificates in the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareBackend")," submodule. See chapter ",(0,o.kt)("a",{parentName:"p",href:"https://gitlab.com/srs/srs-backend#certificates-for-jwt"},"Certificates for JWT")," of the sumodule's README.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the default config for the sensors for the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareBackend"),". See chapter ",(0,o.kt)("a",{parentName:"p",href:"https://gitlab.com/srs/srs-backend#sensors-config-file"},"Sensors config file")," of the sumodule's README.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("a",{parentName:"p",href:"https://k6.io/docs/getting-started/installation/"},"Install")," ",(0,o.kt)("inlineCode",{parentName:"p"},"K6")," for load testing."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69\necho "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list\nsudo apt-get update\nsudo apt-get install k6\n\n')))),(0,o.kt)("h1",{id:"building-and-running-the-pipeline"},"Building and running the pipeline"),(0,o.kt)("h2",{id:"building-and-running-the-pipeling-locally-in-dev"},"Building and running the pipeling locally in DEV"),(0,o.kt)("p",null,"You can make use of the ",(0,o.kt)("a",{parentName:"p",href:"./Makefile"},(0,o.kt)("inlineCode",{parentName:"a"},"Makefile"))," as follow:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"make up\n")),(0,o.kt)("p",null,"See the content of the ",(0,o.kt)("inlineCode",{parentName:"p"},"Makefile")," for all the possible simplified commands."),(0,o.kt)("h2",{id:"building-and-running-the-pipeline-on-the-test-machine"},"Building and running the pipeline on the Test Machine"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Clone the repo on the machine and follow the ",(0,o.kt)("a",{parentName:"p",href:"#initial-setup"},"initial setup")," chapter above to create required certificates.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Run the stack on the machine."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"ssh test-pipeline@192.168.67.29\n\n# on the test machine\ncd repos/srs-pipeline-platform\n\n# build the stack\nmake build\n\n# run the stack\nmake up\n\n# list the services\nmake ps\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the initial admin user that will be able to login into the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareFrontend"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the machine\n# On the machine, run the following script which will ask you some information to create the initial admin user\n./init_admin_user.sh\n")))),(0,o.kt)("h2",{id:"building-and-running-the-pipeline-on-the-prod-machine"},"Building and running the pipeline on the Prod Machine"),(0,o.kt)("h3",{id:"initial-server-ip-configuration"},"Initial server IP configuration"),(0,o.kt)("p",null,"Some client devices (like the door sensors) hardcode the IP of the production pipeline server to which to sent the sensor data. On the server, ",(0,o.kt)("inlineCode",{parentName:"p"},"nginx")," routes the incoming HTTP requests to the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareBackend"),"."),(0,o.kt)("p",null,"The IP address of the production machine is reserverd in the IP list of the router. In case the IP address needs to be changed (replacement of the network card for example), you can update the new IP of the ubuntu server to the desired IP by updating the file ",(0,o.kt)("inlineCode",{parentName:"p"},"/etc/netplan/00-installer-config.yaml")," as follow."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"# File: /etc/netplan/00-installer-config.yaml\nnetwork:\n  ethernets:\n    enp45s0:\n      dhcp4: no\n      addresses:\n        - 192.168.47.144/24\n      gateway4: 192.168.47.1\n      nameservers:\n        addresses: [1.1.1.1]\n  version: 2\n  renderer: networkd\n")),(0,o.kt)("p",null,"Once updated, apply the new plan using the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo netplan apply\n")),(0,o.kt)("h3",{id:"deployment"},"Deployment"),(0,o.kt)("p",null,"Deployment of the pipeline on the production machine is configured through Gitlab CI. See chapter ",(0,o.kt)("a",{parentName:"p",href:"#deployment-using-gitlab-cicd"},"Deployment using Gitlab-CI")," below."),(0,o.kt)("h2",{id:"accessing-the-applications-once-the-services-are-up"},"Accessing the applications once the services are up"),(0,o.kt)("p",null,"In DEV, the services run on your localhost and you can directly access them through your browser, e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"http://localhost:3000")," for the frontend."),(0,o.kt)("p",null,"For the Test/Prod setup, you need to tunnel the needed ports through SSH first. Adapt the user and IP of the server for the Test or Prod setup."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# Example for the production machine\nssh -L 3000:localhost:3000 -L 4000:localhost:4000 -L 8888:localhost:8888 -L 27017:localhost:27017 pipeline@192.168.47.144\n")),(0,o.kt)("p",null,"Then access the applications as follow:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The ",(0,o.kt)("inlineCode",{parentName:"li"},"SensorRecordingSoftwareFrontend")," is ava http://localhost:3000"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Chronograf")," (monitoring): http://localhost:8888"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SensorRecordingSoftware")," API documentation: http://localhost:4000"),(0,o.kt)("li",{parentName:"ul"},"Access MongoDB using MongoPass and connect to: http://localhost:27017")),(0,o.kt)("h2",{id:"load-testing"},"Load testing"),(0,o.kt)("p",null,"We use ",(0,o.kt)("a",{parentName:"p",href:"https://k6.io/docs/"},"K6")," for load testing.\n",(0,o.kt)("a",{parentName:"p",href:"https://k6.io/docs/getting-started/installation/"},"https://k6.io/docs/getting-started/installation/")),(0,o.kt)("p",null,"The load test consists of 50 Virtual Users (VUs) performing a request (sending data) for each sensor type every 500ms. The test runs for a duration of 60s."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"cd ./performance_testing\n./run_test.sh\n")),(0,o.kt)("p",null,"Below is the result of the load test on the test setup."),(0,o.kt)("p",null,"K6 will also output the details of the load test in ",(0,o.kt)("inlineCode",{parentName:"p"},"InfluxDB"),". The result of the load test can be visualized using ",(0,o.kt)("inlineCode",{parentName:"p"},"Chronograf"),". A custom frontend for ",(0,o.kt)("inlineCode",{parentName:"p"},"Chronograf")," is avaiable in ",(0,o.kt)("inlineCode",{parentName:"p"},"./performance_testing/chronograph-dashboard")," and will display the following charts."),(0,o.kt)("h1",{id:"how-to"},"How to"),(0,o.kt)("h2",{id:"configure-an-experiment-and-record-data"},"Configure an experiment and record data"),(0,o.kt)("p",null,"Once the ",(0,o.kt)("inlineCode",{parentName:"p"},"srs Pipeline")," is running, use the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareFrontend")," to configure and experiment and record data."),(0,o.kt)("p",null,"This process consists of:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a new config for the sensors to use:"),(0,o.kt)("p",{parentName:"li"},"This will create a new default config from the file specified with ENV variable: ",(0,o.kt)("inlineCode",{parentName:"p"},"PIPELINE_SENSORS_FILEPATH")," (see file ",(0,o.kt)("inlineCode",{parentName:"p"},".env"),")")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Load the sensors config and edit the sensors you want to use")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a new experiment, assign the config to be used and the owner of the experiment:")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Configure the experiment:"),(0,o.kt)("p",{parentName:"li"},"Here you can define the study protocol:"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"tasks"),(0,o.kt)("li",{parentName:"ul"},"operators"),(0,o.kt)("li",{parentName:"ul"},"participants"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Set the experiment as the active experiment (only one can be active at a time):"),(0,o.kt)("p",{parentName:"li"},'Note that the database must be empty before switching the active experiment! We don\'t want to mix data from different experiment. If the DB is not empty, export the data and backup it (if needed), then drop the content of the DB by visiting the "data management tab".')),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Visualize the sensors activity:")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Start recording:"))),(0,o.kt)("p",null,"Important notes:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A recording can only be started if the DB is empty."),(0,o.kt)("li",{parentName:"ul"},"You can delete the DB from the ",(0,o.kt)("inlineCode",{parentName:"li"},"data management"),"'s page (admin only).")),(0,o.kt)("h2",{id:"exporting-data"},"Exporting data"),(0,o.kt)("p",null,"The data of the active experiment must be exported before starting another experiment. In other words, a new experiment recording can only be started with an empty database (samples collections)."),(0,o.kt)("p",null,"You have 2 options to export data:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"(Preferable way) Use the ",(0,o.kt)("inlineCode",{parentName:"li"},"SensorRecordingSoftwareFrontend")," when the amount of data recorded and to export is reasonable (< 50 GB per export),"),(0,o.kt)("li",{parentName:"ul"},"Use the export script when the amount of data is bigger, note that this requires an access via SSH to the production machine.")),(0,o.kt)("h3",{id:"exporting-data-using-the-sensorrecordingsoftwarefrontend"},"Exporting data using the SensorRecordingSoftwareFrontend"),(0,o.kt)("p",null,"In order to export data, navigate to ",(0,o.kt)("inlineCode",{parentName:"p"},"Admin -> Experiments -> Data management")," and click the ",(0,o.kt)("inlineCode",{parentName:"p"},"Export data active experiment")," button. You will be asked to select the date for which to export the data. It is responsibility of the experiment owner to organize and backup the data exported via the frontend."),(0,o.kt)("p",null,"The exported experiment data is available in folder ",(0,o.kt)("inlineCode",{parentName:"p"},"./data/exports/[EXPERIMENT_UUID]/"),". (The ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareBackend")," exports the data in the ",(0,o.kt)("inlineCode",{parentName:"p"},"./exports")," folder which is mounted in the ",(0,o.kt)("inlineCode",{parentName:"p"},"./data")," folder on the host, alongside the other persistent data.)"),(0,o.kt)("p",null,"The structure of the exported data is as follow:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"[EXPERIMENT_UUID]","/:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"experiment_","[current_datetime]",".json: contains the details of the experiment at the time the export is executed (current_datetime). This includes the sensors used, participants, owner, operators, tasks, ..."),(0,o.kt)("li",{parentName:"ul"},"experiment",(0,o.kt)("em",{parentName:"li"},"log"),"[current_datetime]",".json: contains the part of the logbook associated to the experiment being exported at the time the export is executed (current_datetime)."),(0,o.kt)("li",{parentName:"ul"},"progress",(0,o.kt)("em",{parentName:"li"},"participant"),"[current_datetime]",".json: contains the progress of the participants for the experiment at the time the export is executed (current_datetime)."),(0,o.kt)("li",{parentName:"ul"},"[PARTICIPANT_UUID]",":",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"[date_selected_for_export]","_","[collection_type]","_samples.dump: mongo dump of the samples collections for the date being selected in export.")))))),(0,o.kt)("h3",{id:"exporting-data-using-the-export-script"},"Exporting data using the export script"),(0,o.kt)("p",null,"When there is a lot of data to export, the export feature via the ",(0,o.kt)("inlineCode",{parentName:"p"},"SensorRecordingSoftwareFrontend")," can fail due to requests that timeout to preconfigure the export job. In such case, the script ",(0,o.kt)("a",{parentName:"p",href:"./utils/export_for_participant.sh"},"utils/export_for_participant.sh")," can be used as an alternative."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Important note"),": This script is not complete and only exports the samples collections as single dumps using ",(0,o.kt)("inlineCode",{parentName:"p"},"mongodump")," tool. The script doesn't export the configuration of the experiment, neither the logbook or other metadata related to the experiment. However, it is robust and will work with the database containing hundreds of GB of data."),(0,o.kt)("p",null,"In order to use this script, copy it first to the production machine:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"#\xa0Copy to the production machine\nscp export_for_participant.sh deployer@192.168.47.144:.\n")),(0,o.kt)("p",null,"Then run it by specifying the participant uuid:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"./export_for_participant.sh e33f6d1b-43a7-4a4d-a83e-137db79edd11\n")),(0,o.kt)("p",null,"The data will be exported in the folder ",(0,o.kt)("inlineCode",{parentName:"p"},"./output"),". Note that a single dump is created for each collection, as illustrated below."),(0,o.kt)("p",null,"Then copy the dumps to the ",(0,o.kt)("inlineCode",{parentName:"p"},"RainbowDash")," NAS and don't forget to document the experiment configuration and metadata manually."),(0,o.kt)("h2",{id:"importing-some-dumps"},"Importing some dumps"),(0,o.kt)("p",null,"See folder ",(0,o.kt)("a",{parentName:"p",href:"./utils/import_from_dumps/"},"import_from_dumps")," for an example that show how to restore dumps created during export in a mongo DB."),(0,o.kt)("p",null,"This script loops through all the *.dump files in a directory and restores them in MongoDB."),(0,o.kt)("p",null,"In order to test the script:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Launch a mongo db instance if you don't have one yet. You can use the file ",(0,o.kt)("inlineCode",{parentName:"p"},"./utils/import_from_dumps/docker-compose.yml"),"."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"\ncd ./utils/import_from_dumps/\n\ncp example.env .env\n\ndocker-compose up -d mongo-data-analysis\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Run the import script:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"\n./import_samples_from_dumps.sh ../../data/exports/[EXPERIMENT_UUID]/[PARTICIPANT_UUID]/\n")))),(0,o.kt)("h1",{id:"monitoring"},"Monitoring"),(0,o.kt)("p",null,"We use the TICK stack from ",(0,o.kt)("inlineCode",{parentName:"p"},"InfluxData")," to monitor the production machine and some processes of the Pipeline."),(0,o.kt)("p",null,"The name TICK comes from the intial of those components:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"T"),"elegraf: agent for collecting and reporting metrics and events"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"I"),"nfluxDB: the time series database"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"C"),"hronograph: the interface to manage the entire InfluxData platform"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"K"),"apacitor: real-time streaming data processing engine, also used for alerting")),(0,o.kt)("h2",{id:"chronograf-authentication"},"Chronograf authentication"),(0,o.kt)("p",null,"Authentication and authorization to access the dashboard is done using OAuth 2.0 via ",(0,o.kt)("a",{parentName:"p",href:"https://docs.influxdata.com/chronograf/v1.9/administration/managing-security/#configure-gitlab-authentication"},"Gitlab authentication"),"."),(0,o.kt)("p",null,"In order to enable OAuth via Gitlab, we must create a ",(0,o.kt)("a",{parentName:"p",href:"https://gitlab.com/groups/srs/-/settings/applications"},"Group Application"),". We have actually created two for the group ",(0,o.kt)("inlineCode",{parentName:"p"},"srs"),", one for dev and one for production:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Monitoring Chronograf Production")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Monitoring Chronograf Development"))),(0,o.kt)("p",null,"This will allow Gitlab to perform the authentication and redirect to the correct url to access the ",(0,o.kt)("inlineCode",{parentName:"p"},"Chronograf")," application.\nThe correct application credentials are injected via environment variables."),(0,o.kt)("h2",{id:"tick-stack-configuration"},"TICK Stack configuration"),(0,o.kt)("p",null,"The configuration of the TICK stack is done through configuration files. See the following files:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./srs/tick-stack/influxdb/config/influxdb.conf"},"influxdb.conf")," for ",(0,o.kt)("inlineCode",{parentName:"li"},"InfluxDB")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./srs/tick-stack/telegraf/telegraf.conf"},"telegraf.conf")," for ",(0,o.kt)("inlineCode",{parentName:"li"},"Telegraf")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./srs/tick-stack/kapacitor/config/kapacitor.conf"},"kapacitor.conf")," for ",(0,o.kt)("inlineCode",{parentName:"li"},"Kapacitor")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./srs/tick-stack/chronograf/resources/influxdbconnection.src"},"influxdbconnection.src")," and ",(0,o.kt)("a",{parentName:"li",href:"./srs/tick-stack/chronograf/resources/kapacitorconnection.kap"},"kapacitorconnection.kap")," for ",(0,o.kt)("inlineCode",{parentName:"li"},"Chronograf"))),(0,o.kt)("p",null,"Those files are copied during build by the respectice Dockerfiles."),(0,o.kt)("p",null,"The configuration files use environment variables which are injected by the ",(0,o.kt)("a",{parentName:"p",href:"./docker-compose.yml"},"docker-compose.yml")," file. See variables provided to the services ",(0,o.kt)("inlineCode",{parentName:"p"},"srs-influxdb"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"srs-telegraf"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"srs-chronograf"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"srs-kapacitor"),"."),(0,o.kt)("h2",{id:"kapacitor-alerts-and-notifications"},"Kapacitor alerts and notifications"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Kapacitor")," allows to analyze the time series collected by ",(0,o.kt)("inlineCode",{parentName:"p"},"Telegraf")," and saved in ",(0,o.kt)("inlineCode",{parentName:"p"},"InfluxDB"),". ",(0,o.kt)("inlineCode",{parentName:"p"},"TICKScripts")," are scripts that you can develop to analyze this data and send alerts if some metrics goes over some thresholds. See the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.influxdata.com/kapacitor/v1.5/tick/"},"TICKScript language reference")," for more details."),(0,o.kt)("p",null,"Currently, we have one ",(0,o.kt)("inlineCode",{parentName:"p"},"tickscript")," to monitor the disk size."),(0,o.kt)("p",null,"You can add new tick scripts through the interface but this would be lost when you remove/rebuild the container unless, you mounted the appropriate volumes. We recommand to put your scripts in folder ",(0,o.kt)("inlineCode",{parentName:"p"},"kapacitor/config")," once ready and copy them using the Dockerfile so that they will be available directly when rebuilding the container."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Kapacitor")," is configured to send alerts via SMTP and using SLACK channel (see ",(0,o.kt)("a",{parentName:"p",href:"./srs/tick-stack/kapacitor/config/kapacitor.conf"},"kapacitor.conf"),"). You can test your notification channels via the ",(0,o.kt)("inlineCode",{parentName:"p"},"Chronograf")," interface via Configuration -> My Kapacitor Connection -> Edit button."),(0,o.kt)("p",null,"Here you can send test alerts to check that the credentials for the various channels are properly configured."),(0,o.kt)("p",null,'When pressing "Send test alert" for the SLACK channel, you should receive the following test message:'),(0,o.kt)("h2",{id:"seeing-outputs-logs-of-the-sensors-srs-backend-or-nginx"},"Seeing outputs (logs) of the Sensors srs backend or Nginx"),(0,o.kt)("p",null,"You can check that everything is running fine on the machine and service also by visualizing the output of the ",(0,o.kt)("inlineCode",{parentName:"p"},"srs-backend")," or the logs of nginx."),(0,o.kt)("p",null,"Example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# ssh the machine\n\n# attach the log of srs-backend\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml logs -f --tail=100 srs-backend\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# ssh the machine\n\n# check the nginx logs\ntail -f data/logs/nginx/nginx_access.log\n\n#\xa0or the error log\ntail -f data/logs/nginx/nginx_error.log\n")),(0,o.kt)("h1",{id:"deployment-using-gitlab-cicd"},"Deployment using Gitlab CI/CD"),(0,o.kt)("p",null,"We make use of ",(0,o.kt)("a",{parentName:"p",href:"https://docs.gitlab.com/ee/ci/README.html"},(0,o.kt)("inlineCode",{parentName:"a"},"Gitlab CI/CD"))," for Continuous Integration (CI) and Continuous Deployment/Delivery (CD)."),(0,o.kt)("h2",{id:"process"},"Process"),(0,o.kt)("p",null,"The process consists of two stages:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Build stage:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"During build stage the various docker images required by the project will be built and pushed to our gitlab container registry"))),(0,o.kt)("li",{parentName:"ul"},"Deployment stage:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"For the deployment, we have setup a user called ",(0,o.kt)("inlineCode",{parentName:"li"},"deployer")," on the machine. The stack will be deployed in the home folder of this user, i.e. ",(0,o.kt)("inlineCode",{parentName:"li"},"/home/deployer/"),". During the deployment stage, we will perform the following actions on the production machine:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"create the required files: environement file (.env.prod), certificates (JWT, SSL, ...), docker-compose file, ..."),(0,o.kt)("li",{parentName:"ul"},"pull the required images from our gitlab container registry"),(0,o.kt)("li",{parentName:"ul"},"stop and re-run the stack")))))),(0,o.kt)("h3",{id:"note-on-deployment-strategy"},"Note on deployment strategy"),(0,o.kt)("p",null,"Why do we use SSH to run the deploy commands (see gitlab-ci.yaml) on our own server? From ",(0,o.kt)("a",{parentName:"p",href:"https://www.digitalocean.com/community/tutorials/how-to-set-up-a-continuous-deployment-pipeline-with-gitlab-ci-cd-on-ubuntu-18-04"},"How To Set Up a Continuous Deployment Pipeline with GitLab CI/CD on Ubuntu 18.04"),":"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"It may seem odd to use SSH to run these commands on your server, considering the GitLab runner that executes the commands is the exact same server. Yet it is required, because the runner executes the commands in a Docker container, thus you would deploy inside the container instead of the server if you\u2019d execute the commands without the use of SSH. One could argue that instead of using Docker as a runner executor, you could use the shell executor to run the commands on the host itself. But, that would create a constraint to your pipeline, namely that the runner has to be the same server as the one you want to deploy to. This is not a sustainable and extensible solution because one day you may want to migrate the application to a different server or use a different runner server. In any case it makes sense to use SSH to execute the deployment commands, may it be for technical or migration-related reasons.")),(0,o.kt)("h2",{id:"cicd-setup"},"CI/CD setup"),(0,o.kt)("p",null,"The CI/CD setup consists of:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"creating the user for deployment"),(0,o.kt)("li",{parentName:"ul"},"registering a gitlab runner to execute the jobs for the two stages (build and deploy)"),(0,o.kt)("li",{parentName:"ul"},"configure the jobs in the gitlab-ci.yaml file")),(0,o.kt)("h3",{id:"creating-the-user-for-deployment"},"Creating the user for deployment"),(0,o.kt)("p",null,"Follow the process below to create the user on the server that will be used to deploy the pipeline:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# ssh the machine\nsudo adduser deployer\n")),(0,o.kt)("p",null,"Add the user to the Docker group:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo usermod -aG docker deployer\n")),(0,o.kt)("p",null,"This permits deployer to execute the docker command, which is required to perform the deployment."),(0,o.kt)("p",null,"Then ",(0,o.kt)("a",{parentName:"p",href:"https://docs.gitlab.com/ee/ssh/index.html#generate-an-ssh-key-pair"},"generate a ssh key pair")," and add the public key to the authorized key:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"su deployer\n\n# Generate ssh key pair\nssh-keygen -t rsa -b 2048\n\n# Authorize the SSH key for the deployer user\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n")),(0,o.kt)("p",null,"Copy the content of the id_rsa (private key) as a CI/CD variable (see variable called ID_RSA).\nSee the reference, ",(0,o.kt)("a",{parentName:"p",href:"https://www.digitalocean.com/community/tutorials/how-to-set-up-a-continuous-deployment-pipeline-with-gitlab-ci-cd-on-ubuntu-18-04"},"how To Set Up a Continuous Deployment Pipeline with GitLab CI/CD on Ubuntu 18.04")," for more details."),(0,o.kt)("h3",{id:"register-a-gitlab-runner"},"Register a gitlab runner"),(0,o.kt)("p",null,"The next step is to register a specific runner that will execute our jobs. We've opted to have a the ",(0,o.kt)("inlineCode",{parentName:"p"},"gitlab-runner")," running as a ",(0,o.kt)("a",{parentName:"p",href:"https://docs.gitlab.com/runner/install/docker.html"},"docker container"),". We've decided to register the runner on the Production machine directly, but the runner could be register virtually anywhere, as it will deploy the pipeline by connecting to the production machine over SSH. For example, it is possible to register additional runners on your personnal machine."),(0,o.kt)("p",null,"On the production machine, we will run and register the gitlab runner with the ",(0,o.kt)("inlineCode",{parentName:"p"},"pipeline")," user which has sudo rights so that we can edit the config of the runner after registration. Follow the steps below:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create and run a gitlab-runner service:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'# Copy (if not there yet) the docker-compose file that we will use to run and register a gitlab runner\n# A folder "gitlab_runner" already exists on the home of the pipeline user. If not create the folder before.\nscp docker-compose.gitlab-runner.yml pipeline@192.168.47.144:~/gitlab_runner/.\n\n# SSH the production machine\nssh pipeline@192.168.47.144\n\n# Go to the gitlab_runner folder\ncd gitlab_runner\n\n# Creates the mp-gitlab-runner service\ndocker-compose -f docker-compose.gitlab-runner.yml up -d mp-gitlab-runner\n\n# You can check that the service is running with the following command\ndocker-compose -f docker-compose.gitlab-runner.yml ps\n'))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Register the runner"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the machine\nssh deployer@192.168.47.144\n\ncd ~/gitlab_runner\n\n# Register the runner. When prompted, answer with the questions below\ndocker-compose -f docker-compose.gitlab-runner.yml exec -T mp-gitlab-runner gitlab-runner register\n")),(0,o.kt)("p",{parentName:"li"},"When registering the runner, answer with the following questions:"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},'Gitlab instance: "',(0,o.kt)("a",{parentName:"li",href:"https://gitlab.com/%22"},'https://gitlab.com/"')),(0,o.kt)("li",{parentName:"ul"},"Enter the registration token: [",(0,o.kt)("a",{parentName:"li",href:"https://gitlab.com/srs/srs-backend/-/settings/ci_cd"},"REGISTRATION_TOKEN"),"]"),(0,o.kt)("li",{parentName:"ul"},'Enter a description for the runner: "SRSBackend Machine Runner Deployer"'),(0,o.kt)("li",{parentName:"ul"},'Enter tags for teh runner (comma-separated): "build,deployment_prod"'),(0,o.kt)("li",{parentName:"ul"},"Enter an executor: docker"),(0,o.kt)("li",{parentName:"ul"},'Enter the default Docker image: "docker:stable"')))),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Update the config of the runner:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the machine\nssh deployer@192.168.47.144\n\ncd ~/gitlab_runner\n\n# Stop the runner\ndocker-compose -f docker-compose.gitlab-runner.yml stop mp-gitlab-runner\n\n# Edit the gitlab runner config file (config.toml)\nsudo vi config/gitlab-runner/config.toml\n")),(0,o.kt)("p",{parentName:"li"},"Replace the line ",(0,o.kt)("inlineCode",{parentName:"p"},'volumes = ["/cache"]')," with ",(0,o.kt)("inlineCode",{parentName:"p"},'volumes = ["/cache", "/var/run/docker.sock:/var/run/docker.sock"]')," to fix ",(0,o.kt)("a",{parentName:"p",href:"https://serverfault.com/a/1074274"},"this issue"),")"),(0,o.kt)("p",{parentName:"li"},"After the edit, the ",(0,o.kt)("inlineCode",{parentName:"p"},"config.toml")," file should look like this:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-toml"},'concurrent = 1\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = "SRS Production Machine Runner Deployer"\n  url = "https://gitlab.com/"\n  token = "u_wG5KsHxQUiz5KzHKnH"\n  executor = "docker"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = "docker:stable"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = ["/cache", "/var/run/docker.sock:/var/run/docker.sock"]\n    shm_size = 0\n'))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Restart the runner"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# Still on the prod machine\n\n# Re-run the runner\ndocker-compose -f docker-compose.gitlab-runner.yml up -d mp-gitlab-runner\n")))),(0,o.kt)("p",null,"Upon successfull registration, the gitlab runner will display in the CI/CD settings, under ",(0,o.kt)("a",{parentName:"p",href:"https://gitlab.com/srs/srs-backend/-/settings/ci_cd"},"specific runner"),":"),(0,o.kt)("p",null,"The runner is now ready to execute CI/CD jobs."),(0,o.kt)("h3",{id:"configure-the-gitlab-cicd-variables"},"Configure the Gitlab CI/CD variables"),(0,o.kt)("p",null,"Gitlab-CI will create the ",(0,o.kt)("inlineCode",{parentName:"p"},".env.prod")," that contains the environment variables for production and deploy it on the production machine, before running the docker-compose commands:\n",(0,o.kt)("inlineCode",{parentName:"p"},"docker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml ps"),"."),(0,o.kt)("p",null,"The creation and deployment of the env file is done in the ",(0,o.kt)("inlineCode",{parentName:"p"},"deploy-pipeline-production")," job of the ",(0,o.kt)("inlineCode",{parentName:"p"},"deploy")," stage. See the file ",(0,o.kt)("a",{parentName:"p",href:".gitlab-ci.yml"},"gitlab-ci.yml")," for configuration. This job will create the env file from the ",(0,o.kt)("a",{parentName:"p",href:"https://gitlab.com/srs/srs-backend/-/settings/ci_cd"},"CI/CD variables")," that are configure in Gitlab-CI."),(0,o.kt)("p",null,"The following variables are expected:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"GITLAB_OAUTH_GENERIC_CLIENT_ID"),": the ID of the gitlab group application used for gitlab authentication (in ",(0,o.kt)("inlineCode",{parentName:"li"},"chronograf"),"),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"GITLAB_OAUTH_GENERIC_CLIENT_SECRET"),": the secret of the gitlab group application used for gitlab authentication (in ",(0,o.kt)("inlineCode",{parentName:"li"},"chronograf"),"),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"GITLAB_OAUTH_TOKEN_SECRET"),": a token secret to define between ",(0,o.kt)("inlineCode",{parentName:"li"},"chronograf")," and Gitlab,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"ID_RSA"),": the private ssh key which is used to ssh and deploy the solution on the production machine (the one used by the deployer user),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SERVER_IP"),": the IP of the server on which to deploy the containers (production machine),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SERVER_USER"),": the name of the user which is used to deploy the solution on the production machine, i.e. ",(0,o.kt)("inlineCode",{parentName:"li"},"deployer")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"INFLUXDB_ADMIN_PASSWORD"),": the password for the admin user of the influxdb,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"INFLUXDB_READ_USER_PASSWORD"),": the password for the user with read only access to the influxdb,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"INFLUXDB_WRITE_USER_PASSWORD"),":  the password for the user with write access to the influxdb,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_FROM"),": the email address used to send alerts by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor"),","),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_HOST"),": the SMTP host used to send alerts by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor")," (",(0,o.kt)("inlineCode",{parentName:"li"},"cicero.metanet.ch"),"),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_PASSWORD"),": the password of the email account used to send alerts by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor"),","),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_PORT"),": the port used for SMTP service,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_TO"),": the email recipient to which are sent the alerts (forwarding to specific persons can be configured in ",(0,o.kt)("inlineCode",{parentName:"li"},"cicero.metanet.ch"),")"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"KAP_SMTP_USERNAME"),": the username of the SMTP user,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"MONGODB_USER"),": the root user of the mongodb database,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"MONGODB_PASSWORD"),": the password of root user of the mongodb database,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"MONGODB_DB_NAME"),": the name of the mongodb database,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"NGINX_SSL_CERT_FULLCHAIN"),": the cert fullchain of the SSL wildard certificate,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"NGINX_SSL_CERT_PRIVKEY"),": the private key of the SSL wildcard certificate,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"PIPELINE_SENSORS_FILEPATH"),": relative path from the ",(0,o.kt)("inlineCode",{parentName:"li"},"srs-backend")," to a ",(0,o.kt)("inlineCode",{parentName:"li"},"*.sensors.json")," file"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SRS_BACKEND_API_URL"),": URL of the srs backend service API"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SRS_BACKEND_AUTHENTICATED_WEBSOCKETS_URL"),": URL to open the authenticated websockets (sensors and cameras websockets)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SRS_BACKEND_OPEN_WEBSOCKETS_URL"),": URL to open the non-authenticated websockets (active experiments status websocket)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SLACK_CHANNEL"),": slack channel to which monitoring alerts can be sent by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor"),","),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SLACK_URL"),": the SLACK url used for sending alerts by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor"),","),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SLACK_WORKSPACE"),": the SLACK workspace used for sending alerts by ",(0,o.kt)("inlineCode",{parentName:"li"},"kapacitor"),","),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_JWT_PRIVATE_KEY"),": private key for JWT management,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_JWT_PRIVATE_KEY_FILEPATH"),": path to the private key for the JWT management,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_JWT_PUBLIC_KEY"),": public key for JWT management,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_JWT_PUBLIC_KEY_FILEPATH"),": path to the public key for the JWT management,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_JWT_VALIDITY_LENGTH"),": validity period for the JWT,"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_SMTP_HOST"),": the host used to send mails for the ",(0,o.kt)("inlineCode",{parentName:"li"},"srs-backend")," (bookings),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_SMTP_PASSWORD"),": the password of the account used to send mails for the ",(0,o.kt)("inlineCode",{parentName:"li"},"srs-backend")," (bookings),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_SMTP_USER"),": the user used to send mails for the ",(0,o.kt)("inlineCode",{parentName:"li"},"srs-backend")," (bookings),"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SMS_SMTP_FROM"),": the from address used to send mails for the ",(0,o.kt)("inlineCode",{parentName:"li"},"srs-backend")," (bookings),")),(0,o.kt)("h2",{id:"using-gitlab-ci-pipelines-for-the-deployment"},"Using Gitlab-CI pipelines for the deployment"),(0,o.kt)("p",null,"Once Gitlab-CI is configured properly, when you push your commits to the repository, the commits will appear in the CI/CD pipelines."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"The jobs are configured to be triggered manually"),"."),(0,o.kt)("p",null,"Click on the ",(0,o.kt)("inlineCode",{parentName:"p"},"skipped")," button to display the stages and jobs of the pipeline. You can then use GUI of Gitlab-CI to trigger the jobs."),(0,o.kt)("p",null,"The build stage will build the docker images and push them to the container registry."),(0,o.kt)("p",null,"The deploy stage will:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"create the required files in the home of the deployer user ",(0,o.kt)("inlineCode",{parentName:"li"},"/home/deployer"),", i.e.:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"the ",(0,o.kt)("inlineCode",{parentName:"li"},".env.prod")," file,"),(0,o.kt)("li",{parentName:"ul"},"the ",(0,o.kt)("inlineCode",{parentName:"li"},"docker-compose.yml")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"docker-compose.prod.yml"),","),(0,o.kt)("li",{parentName:"ul"},"the required certificates for JWT management,"))),(0,o.kt)("li",{parentName:"ul"},"replace the version of the the docker images in the image url with the ",(0,o.kt)("inlineCode",{parentName:"li"},"CI_COMMIT_SHORT_SHA"),","),(0,o.kt)("li",{parentName:"ul"},"pull the docker images tagged with the ",(0,o.kt)("inlineCode",{parentName:"li"},"CI_COMMIT_SHORT_SHA")," from the container registry"),(0,o.kt)("li",{parentName:"ul"},"and run the docker-compose stack.")),(0,o.kt)("p",null,"See the content of the ",(0,o.kt)("inlineCode",{parentName:"p"},"gitlab-ci.yml")," file for the jobs definition."),(0,o.kt)("h2",{id:"rollback-to-a-previous-version-using-gitlab-ci"},"Rollback to a previous version using Gitlab-CI"),(0,o.kt)("p",null,"Once the deployment job is completed, if you scroll to the top of the page, you will find the ",(0,o.kt)("inlineCode",{parentName:"p"},"This job is deployed to production")," message. GitLab recognizes that a deployment took place because of the job\u2019s environment section. Click the production link to move over to the production environment."),(0,o.kt)("p",null,"You can also access this page through ",(0,o.kt)("inlineCode",{parentName:"p"},"Deployments")," -> ",(0,o.kt)("inlineCode",{parentName:"p"},"Environments")," which tracks the deployments for the various environments configured."),(0,o.kt)("p",null,"For each deployment there is a re-deploy button available to the very right. A re-deployment will repeat the deploy job of that particular pipeline."),(0,o.kt)("p",null,"Whether a re-deployment works as intended depends on the pipeline configuration, because it will not do more than repeating the deploy job under the same circumstances. Since you have configured to deploy a Docker image using the commit SHA as a tag (see the ",(0,o.kt)("a",{parentName:"p",href:"./docker-compose.prod.yml"},"docker-compose.prod.yml")," file), a re-deployment will work for your pipeline."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Note: Your GitLab container registry may have an expiration policy. The expiration policy regularly removes older images and tags from the container registry. As a consequence, a deployment that is older than the expiration policy would fail to re-deploy, because the Docker image for this commit will have been removed from the registry. You can manage the expiration policy in Settings > CI/CD > Container Registry tag expiration policy. The expiration interval is usually set to something high, like 90 days. But when you run into the case of trying to deploy an image that has been removed from the registry due to the expiration policy, you can solve the problem by re-running the publish job of that particular pipeline as well, which will re-create and push the image for the given commit to registry.")),(0,o.kt)("p",null,"Source: Article on Digital Ocean: ",(0,o.kt)("a",{parentName:"p",href:"https://www.digitalocean.com/community/tutorials/how-to-set-up-a-continuous-deployment-pipeline-with-gitlab-ci-cd-on-ubuntu-18-04#step-7-validating-the-deployment"},"How To Set Up a Continuous Deployment Pipeline with GitLab CI/CD on Ubuntu 18.04")),(0,o.kt)("h2",{id:"cleanup-on-the-production-machine"},"Cleanup on the production machine"),(0,o.kt)("p",null,"From time to time, you can delete the obsolete and dangling images on the production machine to free up space like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the machine\nssh deployer@192.168.47.144\n\n# on the machine\n\n# List the docker images\ndocker images\n\n# Remove the dangling images\ndocker image prune\n")),(0,o.kt)("h1",{id:"backup-and-restore"},"Backup and Restore"),(0,o.kt)("h2",{id:"strategy"},"Strategy"),(0,o.kt)("p",null,"A daily backup of some important collections (users, bookings, experiments, configs, ...) (see ",(0,o.kt)("inlineCode",{parentName:"p"},"./utils/daily_backup.sh")," for the full list) runs daily on the production machine. Note that the daily backup script ",(0,o.kt)("strong",{parentName:"p"},"does not backup any samples collected"),", this is from the responsibility of the owner of the experiment."),(0,o.kt)("p",null,"The full backup process consists of two steps:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"An archive is created by the ",(0,o.kt)("inlineCode",{parentName:"li"},"./utils/daily_backup.sh")," script, which is run as a cronjob by the ",(0,o.kt)("inlineCode",{parentName:"li"},"deployer")," user (see below) at 02:15am."),(0,o.kt)("li",{parentName:"ol"},"The created archive is then copied (rsync) to the ",(0,o.kt)("inlineCode",{parentName:"li"},"Rainbow_Dash NAS")," every day at 05:15am by a backup job configured on the NAS, in shared folder: ",(0,o.kt)("inlineCode",{parentName:"li"},"/DailyBackupProductionMachine/home/deployer/backups"))),(0,o.kt)("h2",{id:"daily-backup-script"},"Daily backup script"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"./utils/daily_backup.sh")," script is aimed to daily backup some collections that have to persists like the users definitions (for users management), experiments definitions, log book, ..."),(0,o.kt)("p",null,"The script is copied on the home folder of the ",(0,o.kt)("inlineCode",{parentName:"p"},"deployer")," user and the following entry can be installed in the crontab to run the backup script daily. This has to be done manually once."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# Copy the daily backup script in the home of the deployer user\nscp utils/daily_backup.sh deployer@192.168.47.144:.\n\n# Login with the deployer user on the machine on which to perform the daily backup\nssh deployer@192.168.47.144\n\n# Edit the crontab\ncrontab -e\n\n# In the crontab editor, enter the following line (will run the script every day at 3:15am)\n15 3 * * * cd /home/deployer/ && ./daily_backup.sh >> ./daily_backup.log 2>&1\n")),(0,o.kt)("p",null,"The daily backup script will create a compressed archive (bz2) of the exported mongo collections in folder  ",(0,o.kt)("inlineCode",{parentName:"p"},"./backups/YYYY/MM")," (e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"./backups/2022/02/2022-02-07_backup.tar.gz"),")."),(0,o.kt)("h2",{id:"backup-job-set-up-on-synology-nas-rainbow_dash"},"Backup job set up on Synology NAS (Rainbow_Dash)"),(0,o.kt)("h3",{id:"pre-requisite-add-a-dedicated-user-for-the-backup-on-the-production-machine"},"Pre-requisite: add a dedicated user for the backup on the production machine"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a dedicated user ",(0,o.kt)("inlineCode",{parentName:"p"},"remotebkp")," on the production machine that we will use to connect and rsync some folders (backup job on the Synology NAS)"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the production machine with a sudo user\nssh pipeline@192.168.47.144\n\n# Create new user account for the user that will connect for the backup\nsudo adduser remotebkp\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Generate a SSH key pair and add the public key ",(0,o.kt)("inlineCode",{parentName:"p"},"remotebkp_id_rsa.pub")," to the authorized keys for the ",(0,o.kt)("inlineCode",{parentName:"p"},"remotebkp")," user on the production machine. This will allow the Synology NAS to ssh and run some rsync commands."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# SSH the production machine with the remotebkp user\nssh remotebkp@194.182.165.107\n\n# Generate a new key pair and name it remotebkp_id_rsa\nssh-keygen -t rsa -b 2048\n\n# Add remotebkp_id_rsa.pub to /home/remotebkp/.ssh/authorized_keys\ncat ~/.ssh/remotebkp_id_rsa.pub >> ~/.ssh/authorized_keys\n")))),(0,o.kt)("h3",{id:"configure-the-backup-job-on-the-nas"},"Configure the backup job on the NAS"),(0,o.kt)("p",null,"Add the backup job on Synology NAS as follows:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"From the DSM start menu, choose ",(0,o.kt)("strong",{parentName:"li"},"Active Backup for Business")),(0,o.kt)("li",{parentName:"ol"},"In ",(0,o.kt)("strong",{parentName:"li"},"File Server"),", click on ",(0,o.kt)("strong",{parentName:"li"},"Add Server")),(0,o.kt)("li",{parentName:"ol"},"Remote server information:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Server address: ",(0,o.kt)("inlineCode",{parentName:"li"},"192.168.47.144")," (IP production machine)"),(0,o.kt)("li",{parentName:"ul"},"Connection mode: ",(0,o.kt)("strong",{parentName:"li"},"rsync shell mode via SSH")),(0,o.kt)("li",{parentName:"ul"},"Port: ",(0,o.kt)("inlineCode",{parentName:"li"},"22")),(0,o.kt)("li",{parentName:"ul"},"Account: ",(0,o.kt)("inlineCode",{parentName:"li"},"remotebkp")),(0,o.kt)("li",{parentName:"ul"},"Authentication method: ",(0,o.kt)("strong",{parentName:"li"},"By SSH key")),(0,o.kt)("li",{parentName:"ul"},"SSH key: Upload ",(0,o.kt)("em",{parentName:"li"},"remotebkp_id_rsa")))),(0,o.kt)("li",{parentName:"ol"},"Create a new task for the server",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Backup type: ",(0,o.kt)("strong",{parentName:"li"},"Incremental")),(0,o.kt)("li",{parentName:"ul"},"Select the files to back up, i.e. ",(0,o.kt)("em",{parentName:"li"},"/home/deployer/backups")))),(0,o.kt)("li",{parentName:"ol"},"Task settings:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Task name: ",(0,o.kt)("inlineCode",{parentName:"li"},"Copy daily backup to Rainbow_Dash NAS")),(0,o.kt)("li",{parentName:"ul"},"Local path: Browse to ",(0,o.kt)("inlineCode",{parentName:"li"},"/DailyBackupProductionMachine")," shared folder (only accessible by admins)"),(0,o.kt)("li",{parentName:"ul"},"Enable schedule: Enabled"),(0,o.kt)("li",{parentName:"ul"},"Run on the following days: ",(0,o.kt)("strong",{parentName:"li"},"Daily")),(0,o.kt)("li",{parentName:"ul"},"First run time: ",(0,o.kt)("strong",{parentName:"li"},"05:15")," (anything sufficiently after the cron job created on the machine)"))),(0,o.kt)("li",{parentName:"ol"},"Review the summary and save the task")),(0,o.kt)("h2",{id:"restoration-from-a-daily-backup-archive"},"Restoration from a daily backup archive"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"./utils/restore_from_backup.sh")," script allows to restore a backup that has been created by the ",(0,o.kt)("inlineCode",{parentName:"p"},"daily_backup.sh")," script. Please make sure the mongo database is empty before running the restore script."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Important note"),": if the databases names are different between the backup and the environment in which you want to restore, edit the ",(0,o.kt)("inlineCode",{parentName:"p"},"nsFrom")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"nsTo")," parameters of the ",(0,o.kt)("inlineCode",{parentName:"p"},"mongorestore")," command in the ",(0,o.kt)("inlineCode",{parentName:"p"},"restore_from_backup.sh")," script. For example, if the mongo dumps created by the ",(0,o.kt)("inlineCode",{parentName:"p"},"daily_backup.sh")," script have been exported from the production db (",(0,o.kt)("inlineCode",{parentName:"p"},"pipeline_prod"),") and that you are trying to restore the dump in your local environment (",(0,o.kt)("inlineCode",{parentName:"p"},"pipeline_dev")," by default), adapt the command in the script as follow:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'mongorestore --uri="mongodb://$MONGODB_USER:$MONGODB_PASSWORD@localhost:27017/?authSource=admin" --archive="$dump_file" --nsFrom="pipeline_prod.*" --nsTo="pipeline_dev.*"\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"#\xa0Extract the archive\ntar -xf 2022-02-07_backup.tar.bz2\n\n# Run the restore script\ncd ./utils\n./restore_from_backup.sh path/to/extracted/2022-02-07_backup\n")),(0,o.kt)("h1",{id:"references"},"References"),(0,o.kt)("p",null,"CI/CD:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://olex.biz/2020/01/deploying-to-docker-nginx-via-gitlab-ci/"},"Deploying applications to the Docker/NGINX server using GitLab CI")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.digitalocean.com/community/tutorials/how-to-set-up-a-continuous-deployment-pipeline-with-gitlab-ci-cd-on-ubuntu-18-04"},"How To Set Up a Continuous Deployment Pipeline with GitLab CI/CD on Ubuntu 18.04")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://techoverflow.net/2021/01/12/how-to-fix-gitlab-ci-error-during-connect-post-http-docker2375-v1-40-auth-dial-tcp-lookup-docker-on-no-such-host/"},"How to fix Gitlab CI error during connect: Post http://docker:2375/v1.40/auth: dial tcp: lookup docker on \u2026 no such host"))),(0,o.kt)("h1",{id:"important-notes"},"Important notes"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"In order to make submodules work correctly in CI/CD jobs, it is needed to use relative URLs for submodules located in the same Gitlab server. See documentation ",(0,o.kt)("a",{parentName:"li",href:"https://docs.gitlab.com/ce/ci/git_submodules.html#configuring-the-gitmodules-file"},"here"),".")),(0,o.kt)("h2",{id:"coding-guidelines"},"Coding guidelines"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"JSON fields: follow snake case")),(0,o.kt)("h1",{id:"useful-commands-cheatsheet"},"Useful commands (cheatsheet)"),(0,o.kt)("h2",{id:"docker"},"Docker"),(0,o.kt)("p",null,"Here below are some usefull commands you can inspire while working with ",(0,o.kt)("inlineCode",{parentName:"p"},"Docker")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"docker-compose"),". Those are example commands you can run on the production environment."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"# List running containers\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml ps\n\n# Display log output from a specific service (e.g. srs-backend)\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml logs -f --tail=100 srs-backend\n\n# Entering a container (e.g. srs-backend)\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml exec srs-backend /bin/sh\n\n# Restarting a service (e.g. srs-backend)\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml restart srs-backend\n")),(0,o.kt)("h2",{id:"working-with-mongodb-container"},"Working with MongoDB container"),(0,o.kt)("p",null,"Enter container"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'#\xa0Enter MongoDB container\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml exec mongo /bin/bash\n\n# From inside contaienr\n\n# Open MongoDB connection\nmongo -u "$MONGO_INITDB_ROOT_USERNAME" -p "$MONGO_INITDB_ROOT_PASSWORD"\n\n# Usefull commands\nshow dbs\nshow collections\n# List users\ndb.users.find()\n# List configs\ndb.experimentconfigs.find()\n')),(0,o.kt)("p",null,"Export dump manually:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"docker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml exec -T mongo sh -c 'mongodump --username=${MONGO_INITDB_ROOT_USERNAME} --password=${MONGO_INITDB_ROOT_PASSWORD} --archive' > ./backups/2021-06-22_09-25.dump\n")),(0,o.kt)("p",null,"Restore dump"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"docker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml exec -T mongo sh -c 'mongorestore --username=${MONGO_INITDB_ROOT_USERNAME} --password=${MONGO_INITDB_ROOT_PASSWORD} --archive' < ./backups/2021-06-22_09-25.dump\n")))}r.isMDXComponent=!0},3808:(e,t,n)=>{n.r(t),n.d(t,{default:()=>c});var a=n(7294),o=n(6010),i=n(2263),r=n(215);const l="heroBanner_qdFl",s="buttons_AeoN";function p(){const{siteConfig:e}=(0,i.Z)();return a.createElement("header",{className:(0,o.Z)("hero hero--primary",l)},a.createElement("div",{className:"container"},a.createElement("h1",{className:"hero__title"},e.title),a.createElement("p",{className:"hero__subtitle"},e.tagline),a.createElement("div",{className:s},a.createElement("h2",null,a.createElement("a",{style:{color:"white",fontWeight:"bold"},target:"_blank",href:n(1990).Z,download:!0},"Download SRS Usability Test PDF"))),a.createElement("div",{className:s},a.createElement("h2",null,a.createElement("a",{style:{color:"white",fontWeight:"bold"},target:"_blank",href:n(3022).Z,download:!0},"Download Protocol SRS Usability Test PDF"))),a.createElement("div",{className:s},a.createElement("h2",null,a.createElement("a",{style:{color:"white",fontWeight:"bold"},target:"_blank",href:n(3056).ZP,download:!0},"Download SRS README"))),a.createElement("div",{className:s},a.createElement("h2",null,"The source-code of this project will be released as soon as SRS's research paper got published."))))}function c(){const{siteConfig:e}=(0,i.Z)();return a.createElement(r.Z,{title:`${e.title}`,description:"Documentation and Tutorials"},a.createElement(p,null),a.createElement("main",null))}},1990:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/files/101e672a-8bf5-4fc4-ad0d-413d687267f5-67816458adc7c67742972d4e3af60547.pdf"},3022:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/files/79e373b0-efeb-4e50-9e81-431487795856-f54a2cf3fa8deba9fb8aca01b1895288.pdf"}}]);